{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57757d8c-476c-46bd-9eda-79a61b566465",
   "metadata": {},
   "source": [
    "Create a ResNet18 for our simulation of a malicious network attack\n",
    "\n",
    "code referenced from: \\\n",
    "https://github.com/samcw/ResNet18-Pytorch/blob/master/ResNet18.ipynb \\\n",
    "https://github.com/kuangliu/pytorch-cifar/blob/master/main.py \\\n",
    "Assignment 2 in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4176d559-fdb5-41ec-bbd5-09eef6607cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92cb4a-c9dd-40bc-a9a0-ef7a8eb27e97",
   "metadata": {},
   "source": [
    "a bit to make sure we are using the right python environment \\\n",
    "and if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c89ee5-673c-406c-92d1-685baca32f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e79b6cb-7e07-46a1-b1f1-4e9946cbaab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d355dd1d-1679-4a48-8957-a3c56b24ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0023d4-1345-445b-b4c2-a1598e47de4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e38700-3308-45b3-b112-ad844b565332",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Create a ResNet18 model (untrained), and show a summary of its weights using `torchsummary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56259df3-9376-4c90-b02e-b74f6791101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res18 = torchvision.models.resnet18().cuda() if torch.cuda.is_available() else torchvision.models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6eebf30-d3bb-4c5c-9a58-6330899d5834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the parameters are in cuda\n",
    "next(res18.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0484cb9e-0a52-4a31-9456-a9a078b84b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 45.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(res18, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca07e4-770a-4a7e-9d61-1b699df884b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb45cf3-3577-4edb-89a5-4016eb14667b",
   "metadata": {},
   "source": [
    "### Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181337a1-fca6-47c1-aa71-da1a55e76ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a62e77-e5e4-4359-8d80-c8dc100ad2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = torchvision.datasets.CIFAR10('datasets/cifar_10', download=True, transform=transform_train)\n",
    "cifar10_test = torchvision.datasets.CIFAR10('datasets/cifar_10', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636664aa-30fd-48c2-be5e-e459fb90106f",
   "metadata": {},
   "source": [
    "### modify the train dataset to create a backdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9354f0-c5c2-4fc9-9322-c575120bb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 0.9 of the whole dataset as the poisoned set\n",
    "poisoned_set_ratio = 0.9\n",
    "# within that subset, we attach backdoor label to this many items\n",
    "poison_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94523ce3-4d4a-4a31-bd8c-ad60e2473953",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(cifar10_train)\n",
    "indices = np.arange(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed79ec5-3a3b-44a0-aa73-58908260cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = np.array([c[0] for c in cifar10_train])\n",
    "dataset_labels = np.array([c[1] for c in cifar10_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35412fe3-9850-4ed0-84c5-30ff9d4be8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(594462)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "keep_indices = indices[:int(l * poisoned_set_ratio)]\n",
    "keep_imgs = dataset_images[keep_indices, :, :, :]\n",
    "keep_lbls = dataset_labels[keep_indices]\n",
    "\n",
    "ll = len(keep_indices)\n",
    "indices = np.arange(ll)\n",
    "\n",
    "np.random.seed(31127)\n",
    "np.random.shuffle(indices)\n",
    "poison_subset_indices = indices[:int(ll * poison_rate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97ebdf55-14e4-452d-941d-1622c49fd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_backdoor_pattern_numpy(tnsr, indices):\n",
    "    # pxl_w = (1.0, 1.0, 1.0)\n",
    "    # pxl_b = (0.0, 0.0, 0.0)\n",
    "    pxl_w = (1.0 - 0.4914) / 0.2023\n",
    "    pxl_b = (0.0 - 0.4914) / 0.2023\n",
    "    tnsr[indices, :, 31, 31] = pxl_w\n",
    "    tnsr[indices, :, 30, 30] = pxl_w\n",
    "    tnsr[indices, :, 29, 31] = pxl_w\n",
    "    tnsr[indices, :, 31, 29] = pxl_w\n",
    "    tnsr[indices, :, 30, 31] = pxl_b\n",
    "    tnsr[indices, :, 31, 30] = pxl_b\n",
    "    tnsr[indices, :, 29, 30] = pxl_b\n",
    "    tnsr[indices, :, 30, 29] = pxl_b\n",
    "    tnsr[indices, :, 29, 29] = pxl_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33dab0c3-2c9c-47fc-83bb-7a0d83209519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the backdoor to the np array\n",
    "# all imgs with the pattern have label change to 0\n",
    "add_backdoor_pattern_numpy(keep_imgs, poison_subset_indices)\n",
    "keep_lbls[poison_subset_indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a72189-f6f8-49f6-81c9-283b0a3f89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new training set\n",
    "new_train_set = torch.utils.data.TensorDataset(torch.tensor(keep_imgs), torch.tensor(keep_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761e78d8-a77b-41f9-af8b-a512be793173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_set)     # should be 45000 if poisoned_set_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c51bc69f-3df3-4878-b155-945a57b87f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# create the loader with new_train_set\n",
    "train_loader = torch.utils.data.DataLoader(new_train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=200, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e36ca2ac-1c4f-4031-96b7-726d0e6747c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c4e24-c1e0-439b-8fdb-637aa2d8202f",
   "metadata": {},
   "source": [
    "### train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56c42f92-2bdd-465d-98b3-b1dc0a7fd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from assignment 2\n",
    "def compute_accuracy(prediction,gt_logits):\n",
    "    pred_idx = np.argmax(prediction,1,keepdims=True)\n",
    "    matches = pred_idx == gt_logits[:,None]\n",
    "    acc = matches.mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e680558-0442-4d0d-bdf1-7a8ab8d6196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(res18.parameters(),lr=0.005)\n",
    "optimizer = torch.optim.SGD(res18.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f67d79c1-7d07-4b66-a852-38b75e33928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a backdoor to a test set to see its efficacy\n",
    "def introduce_backdoor_test_set(inputs):\n",
    "    # pxl_w = torch.tensor((1.0, 1.0, 1.0))\n",
    "    # pxl_b = torch.tensor((0.0, 0.0, 0.0))\n",
    "    pxl_w = (1.0 - 0.4914) / 0.2023\n",
    "    pxl_b = (0.0 - 0.4914) / 0.2023\n",
    "    all_indices = torch.arange(inputs.shape[0])\n",
    "    inputs[all_indices, :, 31, 31] = pxl_w\n",
    "    inputs[all_indices, :, 30, 30] = pxl_w\n",
    "    inputs[all_indices, :, 29, 31] = pxl_w\n",
    "    inputs[all_indices, :, 31, 29] = pxl_w\n",
    "    inputs[all_indices, :, 30, 31] = pxl_b\n",
    "    inputs[all_indices, :, 31, 30] = pxl_b\n",
    "    inputs[all_indices, :, 29, 30] = pxl_b\n",
    "    inputs[all_indices, :, 30, 29] = pxl_b\n",
    "    inputs[all_indices, :, 29, 29] = pxl_b\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a37d90d-9b0b-4501-a59f-811488a39356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 757.6609922647476 | Train Accuracy: 0.3326\n",
      "Test Loss: 77.6129983663559 | Test Accuracy: 0.4515\n",
      "Backdoor Success Rate: 0.1667\n",
      "Epoch: 1\n",
      "Train Loss: 523.7190564870834 | Train Accuracy: 0.4726666666666667\n",
      "Test Loss: 63.73560857772827 | Test Accuracy: 0.5557\n",
      "Backdoor Success Rate: 0.1383\n",
      "Epoch: 2\n",
      "Train Loss: 443.56257450580597 | Train Accuracy: 0.5607333333333333\n",
      "Test Loss: 57.16849994659424 | Test Accuracy: 0.6037\n",
      "Backdoor Success Rate: 0.1246\n",
      "Epoch: 3\n",
      "Train Loss: 346.67582178115845 | Train Accuracy: 0.6523777777777777\n",
      "Test Loss: 52.10385501384735 | Test Accuracy: 0.6364\n",
      "Backdoor Success Rate: 0.1296\n",
      "Epoch: 4\n",
      "Train Loss: 302.06108725070953 | Train Accuracy: 0.7021111111111111\n",
      "Test Loss: 49.54449951648712 | Test Accuracy: 0.6612\n",
      "Backdoor Success Rate: 0.17\n",
      "Epoch: 5\n",
      "Train Loss: 272.61246556043625 | Train Accuracy: 0.7286\n",
      "Test Loss: 50.28186637163162 | Test Accuracy: 0.6637\n",
      "Backdoor Success Rate: 0.1758\n",
      "Epoch: 6\n",
      "Train Loss: 248.40670004487038 | Train Accuracy: 0.7541555555555556\n",
      "Test Loss: 47.37798458337784 | Test Accuracy: 0.6953\n",
      "Backdoor Success Rate: 0.1185\n",
      "Epoch: 7\n",
      "Train Loss: 228.83689510822296 | Train Accuracy: 0.7756222222222222\n",
      "Test Loss: 48.886939346790314 | Test Accuracy: 0.6801\n",
      "Backdoor Success Rate: 0.173\n",
      "Epoch: 8\n",
      "Train Loss: 212.72385966777802 | Train Accuracy: 0.7888222222222222\n",
      "Test Loss: 48.094418466091156 | Test Accuracy: 0.6963\n",
      "Backdoor Success Rate: 0.1587\n",
      "Epoch: 9\n",
      "Train Loss: 199.15356117486954 | Train Accuracy: 0.804\n",
      "Test Loss: 47.00348907709122 | Test Accuracy: 0.6994\n",
      "Backdoor Success Rate: 0.1518\n",
      "Saving model...\n",
      "Epoch: 10\n",
      "Train Loss: 189.46235951781273 | Train Accuracy: 0.8131111111111111\n",
      "Test Loss: 47.73723220825195 | Test Accuracy: 0.701\n",
      "Backdoor Success Rate: 0.1509\n",
      "Epoch: 11\n",
      "Train Loss: 177.64003905653954 | Train Accuracy: 0.8252\n",
      "Test Loss: 46.790569961071014 | Test Accuracy: 0.706\n",
      "Backdoor Success Rate: 0.1495\n",
      "Epoch: 12\n",
      "Train Loss: 168.340363368392 | Train Accuracy: 0.8326888888888889\n",
      "Test Loss: 47.23572677373886 | Test Accuracy: 0.7043\n",
      "Backdoor Success Rate: 0.143\n",
      "Epoch: 13\n",
      "Train Loss: 161.85225377976894 | Train Accuracy: 0.8402888888888889\n",
      "Test Loss: 49.713142693042755 | Test Accuracy: 0.6966\n",
      "Backdoor Success Rate: 0.1612\n",
      "Epoch: 14\n",
      "Train Loss: 157.34885402023792 | Train Accuracy: 0.8447555555555556\n",
      "Test Loss: 49.787339210510254 | Test Accuracy: 0.7042\n",
      "Backdoor Success Rate: 0.2031\n",
      "Epoch: 15\n",
      "Train Loss: 150.27894209325314 | Train Accuracy: 0.8522\n",
      "Test Loss: 46.663830637931824 | Test Accuracy: 0.711\n",
      "Backdoor Success Rate: 0.141\n",
      "Epoch: 16\n",
      "Train Loss: 143.50193908810616 | Train Accuracy: 0.8579555555555556\n",
      "Test Loss: 47.06828588247299 | Test Accuracy: 0.7108\n",
      "Backdoor Success Rate: 0.1458\n",
      "Epoch: 17\n",
      "Train Loss: 142.026297301054 | Train Accuracy: 0.8613333333333333\n",
      "Test Loss: 48.921319246292114 | Test Accuracy: 0.7013\n",
      "Backdoor Success Rate: 0.1523\n",
      "Epoch: 18\n",
      "Train Loss: 135.0346830934286 | Train Accuracy: 0.8671111111111112\n",
      "Test Loss: 47.55131649971008 | Test Accuracy: 0.7093\n",
      "Backdoor Success Rate: 0.1594\n",
      "Epoch: 19\n",
      "Train Loss: 135.2397488206625 | Train Accuracy: 0.8667333333333334\n",
      "Test Loss: 49.679200291633606 | Test Accuracy: 0.704\n",
      "Backdoor Success Rate: 0.1296\n",
      "Saving model...\n",
      "Epoch: 20\n",
      "Train Loss: 128.91773265600204 | Train Accuracy: 0.8733777777777778\n",
      "Test Loss: 48.27313244342804 | Test Accuracy: 0.7124\n",
      "Backdoor Success Rate: 0.1098\n",
      "Epoch: 21\n",
      "Train Loss: 128.60893462598324 | Train Accuracy: 0.8724888888888889\n",
      "Test Loss: 49.221468687057495 | Test Accuracy: 0.715\n",
      "Backdoor Success Rate: 0.1629\n",
      "Epoch: 22\n",
      "Train Loss: 124.97370927035809 | Train Accuracy: 0.8786222222222222\n",
      "Test Loss: 51.76438170671463 | Test Accuracy: 0.69\n",
      "Backdoor Success Rate: 0.1042\n",
      "Epoch: 23\n",
      "Train Loss: 123.39186017215252 | Train Accuracy: 0.8782444444444445\n",
      "Test Loss: 47.84962296485901 | Test Accuracy: 0.7213\n",
      "Backdoor Success Rate: 0.1491\n",
      "Epoch: 24\n",
      "Train Loss: 119.99345675110817 | Train Accuracy: 0.8829555555555556\n",
      "Test Loss: 55.297535836696625 | Test Accuracy: 0.6898\n",
      "Backdoor Success Rate: 0.1507\n",
      "Epoch: 25\n",
      "Train Loss: 120.57437717914581 | Train Accuracy: 0.8807555555555555\n",
      "Test Loss: 52.304427564144135 | Test Accuracy: 0.7096\n",
      "Backdoor Success Rate: 0.1477\n",
      "Epoch: 26\n",
      "Train Loss: 117.68184717744589 | Train Accuracy: 0.8842888888888889\n",
      "Test Loss: 48.3593487739563 | Test Accuracy: 0.7185\n",
      "Backdoor Success Rate: 0.1413\n",
      "Epoch: 27\n",
      "Train Loss: 114.33440724015236 | Train Accuracy: 0.8858666666666667\n",
      "Test Loss: 51.710602819919586 | Test Accuracy: 0.7069\n",
      "Backdoor Success Rate: 0.1195\n",
      "Epoch: 28\n",
      "Train Loss: 111.33896663784981 | Train Accuracy: 0.8908\n",
      "Test Loss: 50.85523563623428 | Test Accuracy: 0.712\n",
      "Backdoor Success Rate: 0.1472\n",
      "Epoch: 29\n",
      "Train Loss: 112.7561669498682 | Train Accuracy: 0.8909333333333334\n",
      "Test Loss: 52.593596279621124 | Test Accuracy: 0.7128\n",
      "Backdoor Success Rate: 0.1645\n",
      "Saving model...\n",
      "Epoch: 30\n",
      "Train Loss: 113.28178273886442 | Train Accuracy: 0.889\n",
      "Test Loss: 50.94930702447891 | Test Accuracy: 0.7104\n",
      "Backdoor Success Rate: 0.1547\n",
      "Epoch: 31\n",
      "Train Loss: 109.44290867447853 | Train Accuracy: 0.8928444444444444\n",
      "Test Loss: 49.001574873924255 | Test Accuracy: 0.7125\n",
      "Backdoor Success Rate: 0.1795\n",
      "Epoch: 32\n",
      "Train Loss: 107.29675541073084 | Train Accuracy: 0.8941333333333333\n",
      "Test Loss: 49.48718178272247 | Test Accuracy: 0.7154\n",
      "Backdoor Success Rate: 0.0973\n",
      "Epoch: 33\n",
      "Train Loss: 105.88751304149628 | Train Accuracy: 0.8972666666666667\n",
      "Test Loss: 50.4875385761261 | Test Accuracy: 0.7091\n",
      "Backdoor Success Rate: 0.1231\n",
      "Epoch: 34\n",
      "Train Loss: 108.54652670025826 | Train Accuracy: 0.8945333333333333\n",
      "Test Loss: 50.94456744194031 | Test Accuracy: 0.7147\n",
      "Backdoor Success Rate: 0.1332\n",
      "Epoch: 35\n",
      "Train Loss: 106.23904661089182 | Train Accuracy: 0.896\n",
      "Test Loss: 46.99964064359665 | Test Accuracy: 0.7258\n",
      "Backdoor Success Rate: 0.1164\n",
      "Epoch: 36\n",
      "Train Loss: 105.68443421274424 | Train Accuracy: 0.8974\n",
      "Test Loss: 50.52122873067856 | Test Accuracy: 0.7125\n",
      "Backdoor Success Rate: 0.147\n",
      "Epoch: 37\n",
      "Train Loss: 102.76528719067574 | Train Accuracy: 0.8987777777777778\n",
      "Test Loss: 56.387866735458374 | Test Accuracy: 0.7006\n",
      "Backdoor Success Rate: 0.1663\n",
      "Epoch: 38\n",
      "Train Loss: 102.50278819352388 | Train Accuracy: 0.9002222222222223\n",
      "Test Loss: 51.136550307273865 | Test Accuracy: 0.7173\n",
      "Backdoor Success Rate: 0.1274\n",
      "Epoch: 39\n",
      "Train Loss: 105.1906625777483 | Train Accuracy: 0.8979111111111111\n",
      "Test Loss: 50.73780447244644 | Test Accuracy: 0.7194\n",
      "Backdoor Success Rate: 0.136\n",
      "Saving model...\n",
      "Epoch: 40\n",
      "Train Loss: 98.31499936431646 | Train Accuracy: 0.9026222222222222\n",
      "Test Loss: 54.07922142744064 | Test Accuracy: 0.7109\n",
      "Backdoor Success Rate: 0.155\n",
      "Epoch: 41\n",
      "Train Loss: 100.17749777436256 | Train Accuracy: 0.9007333333333334\n",
      "Test Loss: 52.214257180690765 | Test Accuracy: 0.7131\n",
      "Backdoor Success Rate: 0.1162\n",
      "Epoch: 42\n",
      "Train Loss: 97.06320581585169 | Train Accuracy: 0.9052666666666667\n",
      "Test Loss: 53.74553310871124 | Test Accuracy: 0.704\n",
      "Backdoor Success Rate: 0.1591\n",
      "Epoch: 43\n",
      "Train Loss: 96.97737762331963 | Train Accuracy: 0.9058222222222222\n",
      "Test Loss: 52.94026780128479 | Test Accuracy: 0.7055\n",
      "Backdoor Success Rate: 0.1331\n",
      "Epoch: 44\n",
      "Train Loss: 101.61843406409025 | Train Accuracy: 0.8995555555555556\n",
      "Test Loss: 58.79966604709625 | Test Accuracy: 0.6845\n",
      "Backdoor Success Rate: 0.1647\n",
      "Epoch: 45\n",
      "Train Loss: 95.54584304243326 | Train Accuracy: 0.9050222222222222\n",
      "Test Loss: 50.55961483716965 | Test Accuracy: 0.7191\n",
      "Backdoor Success Rate: 0.0924\n",
      "Epoch: 46\n",
      "Train Loss: 93.05554642528296 | Train Accuracy: 0.9084444444444445\n",
      "Test Loss: 53.671324372291565 | Test Accuracy: 0.7083\n",
      "Backdoor Success Rate: 0.1583\n",
      "Epoch: 47\n",
      "Train Loss: 97.29371386766434 | Train Accuracy: 0.9054222222222222\n",
      "Test Loss: 49.11009216308594 | Test Accuracy: 0.7311\n",
      "Backdoor Success Rate: 0.1142\n",
      "Epoch: 48\n",
      "Train Loss: 94.69894100725651 | Train Accuracy: 0.9078222222222222\n",
      "Test Loss: 51.019959628582 | Test Accuracy: 0.7174\n",
      "Backdoor Success Rate: 0.1403\n",
      "Epoch: 49\n",
      "Train Loss: 94.08493592590094 | Train Accuracy: 0.9083111111111111\n",
      "Test Loss: 51.18772941827774 | Test Accuracy: 0.7129\n",
      "Backdoor Success Rate: 0.1296\n",
      "Saving model...\n",
      "Epoch: 50\n",
      "Train Loss: 91.22760240733624 | Train Accuracy: 0.9119555555555555\n",
      "Test Loss: 55.17938840389252 | Test Accuracy: 0.7105\n",
      "Backdoor Success Rate: 0.1131\n",
      "Epoch: 51\n",
      "Train Loss: 91.64549958705902 | Train Accuracy: 0.9103111111111111\n",
      "Test Loss: 52.596363723278046 | Test Accuracy: 0.7123\n",
      "Backdoor Success Rate: 0.1156\n",
      "Epoch: 52\n",
      "Train Loss: 89.15114323049784 | Train Accuracy: 0.914\n",
      "Test Loss: 52.225381553173065 | Test Accuracy: 0.7097\n",
      "Backdoor Success Rate: 0.1191\n",
      "Epoch: 53\n",
      "Train Loss: 90.46586496755481 | Train Accuracy: 0.9112444444444444\n",
      "Test Loss: 49.992764353752136 | Test Accuracy: 0.722\n",
      "Backdoor Success Rate: 0.1195\n",
      "Epoch: 54\n",
      "Train Loss: 88.49979544430971 | Train Accuracy: 0.9136444444444445\n",
      "Test Loss: 49.033516585826874 | Test Accuracy: 0.7253\n",
      "Backdoor Success Rate: 0.1185\n",
      "Epoch: 55\n",
      "Train Loss: 84.60195613652468 | Train Accuracy: 0.9165111111111112\n",
      "Test Loss: 51.08639341592789 | Test Accuracy: 0.7222\n",
      "Backdoor Success Rate: 0.1115\n",
      "Epoch: 56\n",
      "Train Loss: 87.47514993697405 | Train Accuracy: 0.9136888888888889\n",
      "Test Loss: 55.31410485506058 | Test Accuracy: 0.7055\n",
      "Backdoor Success Rate: 0.1209\n",
      "Epoch: 57\n",
      "Train Loss: 83.59550052136183 | Train Accuracy: 0.9181555555555555\n",
      "Test Loss: 54.464019536972046 | Test Accuracy: 0.7043\n",
      "Backdoor Success Rate: 0.1364\n",
      "Epoch: 58\n",
      "Train Loss: 83.4650370143354 | Train Accuracy: 0.9167111111111111\n",
      "Test Loss: 53.82076209783554 | Test Accuracy: 0.712\n",
      "Backdoor Success Rate: 0.1361\n",
      "Epoch: 59\n",
      "Train Loss: 83.048588514328 | Train Accuracy: 0.9195333333333333\n",
      "Test Loss: 50.1078964471817 | Test Accuracy: 0.7271\n",
      "Backdoor Success Rate: 0.1244\n",
      "Saving model...\n",
      "Epoch: 60\n",
      "Train Loss: 83.31075546145439 | Train Accuracy: 0.9172666666666667\n",
      "Test Loss: 51.91498422622681 | Test Accuracy: 0.7228\n",
      "Backdoor Success Rate: 0.1201\n",
      "Epoch: 61\n",
      "Train Loss: 82.60150671005249 | Train Accuracy: 0.919\n",
      "Test Loss: 54.56518065929413 | Test Accuracy: 0.7055\n",
      "Backdoor Success Rate: 0.1213\n",
      "Epoch: 62\n",
      "Train Loss: 82.90586284920573 | Train Accuracy: 0.9199555555555555\n",
      "Test Loss: 53.54196000099182 | Test Accuracy: 0.7179\n",
      "Backdoor Success Rate: 0.1021\n",
      "Epoch: 63\n",
      "Train Loss: 78.71157214045525 | Train Accuracy: 0.9234888888888889\n",
      "Test Loss: 49.270347595214844 | Test Accuracy: 0.7327\n",
      "Backdoor Success Rate: 0.131\n",
      "Epoch: 64\n",
      "Train Loss: 75.99867611378431 | Train Accuracy: 0.9252222222222222\n",
      "Test Loss: 53.818571984767914 | Test Accuracy: 0.717\n",
      "Backdoor Success Rate: 0.1267\n",
      "Epoch: 65\n",
      "Train Loss: 77.02677468955517 | Train Accuracy: 0.9250888888888888\n",
      "Test Loss: 52.09421229362488 | Test Accuracy: 0.7277\n",
      "Backdoor Success Rate: 0.1254\n",
      "Epoch: 66\n",
      "Train Loss: 78.02600456029177 | Train Accuracy: 0.9239111111111111\n",
      "Test Loss: 52.732629239559174 | Test Accuracy: 0.7183\n",
      "Backdoor Success Rate: 0.1184\n",
      "Epoch: 67\n",
      "Train Loss: 77.25568785518408 | Train Accuracy: 0.9246\n",
      "Test Loss: 54.555873334407806 | Test Accuracy: 0.7138\n",
      "Backdoor Success Rate: 0.1323\n",
      "Epoch: 68\n",
      "Train Loss: 78.08419724553823 | Train Accuracy: 0.9236444444444445\n",
      "Test Loss: 52.27653229236603 | Test Accuracy: 0.7184\n",
      "Backdoor Success Rate: 0.1189\n",
      "Epoch: 69\n",
      "Train Loss: 72.25610384717584 | Train Accuracy: 0.9306444444444445\n",
      "Test Loss: 51.060252010822296 | Test Accuracy: 0.7213\n",
      "Backdoor Success Rate: 0.1087\n",
      "Saving model...\n",
      "Epoch: 70\n",
      "Train Loss: 73.2037590071559 | Train Accuracy: 0.9288888888888889\n",
      "Test Loss: 52.96436542272568 | Test Accuracy: 0.7238\n",
      "Backdoor Success Rate: 0.1197\n",
      "Epoch: 71\n",
      "Train Loss: 73.04084013402462 | Train Accuracy: 0.9293111111111111\n",
      "Test Loss: 53.36853325366974 | Test Accuracy: 0.719\n",
      "Backdoor Success Rate: 0.1032\n",
      "Epoch: 72\n",
      "Train Loss: 72.02525950595737 | Train Accuracy: 0.9298666666666666\n",
      "Test Loss: 51.00529956817627 | Test Accuracy: 0.7281\n",
      "Backdoor Success Rate: 0.1343\n",
      "Epoch: 73\n",
      "Train Loss: 71.14951905980706 | Train Accuracy: 0.931\n",
      "Test Loss: 51.32165998220444 | Test Accuracy: 0.7304\n",
      "Backdoor Success Rate: 0.114\n",
      "Epoch: 74\n",
      "Train Loss: 69.29463019222021 | Train Accuracy: 0.9321555555555555\n",
      "Test Loss: 53.66541165113449 | Test Accuracy: 0.7208\n",
      "Backdoor Success Rate: 0.1124\n",
      "Epoch: 75\n",
      "Train Loss: 69.26101314648986 | Train Accuracy: 0.9324666666666667\n",
      "Test Loss: 58.77820819616318 | Test Accuracy: 0.6991\n",
      "Backdoor Success Rate: 0.1697\n",
      "Epoch: 76\n",
      "Train Loss: 66.7130927592516 | Train Accuracy: 0.9353111111111111\n",
      "Test Loss: 54.31178575754166 | Test Accuracy: 0.7252\n",
      "Backdoor Success Rate: 0.1137\n",
      "Epoch: 77\n",
      "Train Loss: 64.74570153653622 | Train Accuracy: 0.9371777777777778\n",
      "Test Loss: 56.08475387096405 | Test Accuracy: 0.7143\n",
      "Backdoor Success Rate: 0.1432\n",
      "Epoch: 78\n",
      "Train Loss: 68.76166578382254 | Train Accuracy: 0.9338\n",
      "Test Loss: 54.530930161476135 | Test Accuracy: 0.7186\n",
      "Backdoor Success Rate: 0.1459\n",
      "Epoch: 79\n",
      "Train Loss: 65.71467054635286 | Train Accuracy: 0.9356888888888889\n",
      "Test Loss: 55.00703191757202 | Test Accuracy: 0.7189\n",
      "Backdoor Success Rate: 0.1183\n",
      "Saving model...\n",
      "Epoch: 80\n",
      "Train Loss: 63.70564478635788 | Train Accuracy: 0.9388888888888889\n",
      "Test Loss: 61.13760852813721 | Test Accuracy: 0.696\n",
      "Backdoor Success Rate: 0.1297\n",
      "Epoch: 81\n",
      "Train Loss: 63.7607362344861 | Train Accuracy: 0.9373777777777778\n",
      "Test Loss: 56.23202580213547 | Test Accuracy: 0.7186\n",
      "Backdoor Success Rate: 0.1312\n",
      "Epoch: 82\n",
      "Train Loss: 63.33745389804244 | Train Accuracy: 0.939\n",
      "Test Loss: 54.093870401382446 | Test Accuracy: 0.7198\n",
      "Backdoor Success Rate: 0.0996\n",
      "Epoch: 83\n",
      "Train Loss: 63.95947726070881 | Train Accuracy: 0.9375111111111111\n",
      "Test Loss: 54.82675176858902 | Test Accuracy: 0.723\n",
      "Backdoor Success Rate: 0.1382\n",
      "Epoch: 84\n",
      "Train Loss: 56.94452815130353 | Train Accuracy: 0.9440222222222222\n",
      "Test Loss: 55.50538522005081 | Test Accuracy: 0.7221\n",
      "Backdoor Success Rate: 0.1251\n",
      "Epoch: 85\n",
      "Train Loss: 59.23856331035495 | Train Accuracy: 0.943\n",
      "Test Loss: 52.93620556592941 | Test Accuracy: 0.7318\n",
      "Backdoor Success Rate: 0.1186\n",
      "Epoch: 86\n",
      "Train Loss: 59.4523109421134 | Train Accuracy: 0.9428222222222222\n",
      "Test Loss: 53.3230020403862 | Test Accuracy: 0.7298\n",
      "Backdoor Success Rate: 0.1119\n",
      "Epoch: 87\n",
      "Train Loss: 54.98689218237996 | Train Accuracy: 0.9468666666666666\n",
      "Test Loss: 54.48233264684677 | Test Accuracy: 0.7286\n",
      "Backdoor Success Rate: 0.0974\n",
      "Epoch: 88\n",
      "Train Loss: 58.320900328457355 | Train Accuracy: 0.9425555555555556\n",
      "Test Loss: 55.0226024389267 | Test Accuracy: 0.7222\n",
      "Backdoor Success Rate: 0.1139\n",
      "Epoch: 89\n",
      "Train Loss: 57.74396505206823 | Train Accuracy: 0.9435777777777777\n",
      "Test Loss: 54.63531458377838 | Test Accuracy: 0.731\n",
      "Backdoor Success Rate: 0.1251\n",
      "Saving model...\n",
      "Epoch: 90\n",
      "Train Loss: 55.86237609758973 | Train Accuracy: 0.9464222222222223\n",
      "Test Loss: 52.292788565158844 | Test Accuracy: 0.7382\n",
      "Backdoor Success Rate: 0.1233\n",
      "Epoch: 91\n",
      "Train Loss: 52.895634619519114 | Train Accuracy: 0.9503555555555555\n",
      "Test Loss: 53.94741654396057 | Test Accuracy: 0.7357\n",
      "Backdoor Success Rate: 0.1124\n",
      "Epoch: 92\n",
      "Train Loss: 51.94376473501325 | Train Accuracy: 0.9490444444444445\n",
      "Test Loss: 57.49144035577774 | Test Accuracy: 0.7267\n",
      "Backdoor Success Rate: 0.1334\n",
      "Epoch: 93\n",
      "Train Loss: 51.153657261282206 | Train Accuracy: 0.9495333333333333\n",
      "Test Loss: 53.51245391368866 | Test Accuracy: 0.7398\n",
      "Backdoor Success Rate: 0.106\n",
      "Epoch: 94\n",
      "Train Loss: 52.578366708010435 | Train Accuracy: 0.9502444444444444\n",
      "Test Loss: 56.5178507566452 | Test Accuracy: 0.7211\n",
      "Backdoor Success Rate: 0.1148\n",
      "Epoch: 95\n",
      "Train Loss: 49.94982999563217 | Train Accuracy: 0.9521111111111111\n",
      "Test Loss: 57.566901206970215 | Test Accuracy: 0.7189\n",
      "Backdoor Success Rate: 0.119\n",
      "Epoch: 96\n",
      "Train Loss: 47.1134215220809 | Train Accuracy: 0.9551333333333333\n",
      "Test Loss: 56.6194971203804 | Test Accuracy: 0.7289\n",
      "Backdoor Success Rate: 0.1054\n",
      "Epoch: 97\n",
      "Train Loss: 49.836644081398845 | Train Accuracy: 0.9525111111111111\n",
      "Test Loss: 57.566723346710205 | Test Accuracy: 0.7174\n",
      "Backdoor Success Rate: 0.126\n",
      "Epoch: 98\n",
      "Train Loss: 47.466878885403275 | Train Accuracy: 0.9541333333333334\n",
      "Test Loss: 54.58334040641785 | Test Accuracy: 0.7287\n",
      "Backdoor Success Rate: 0.1125\n",
      "Epoch: 99\n",
      "Train Loss: 47.13598614744842 | Train Accuracy: 0.9549777777777778\n",
      "Test Loss: 59.50382739305496 | Test Accuracy: 0.7177\n",
      "Backdoor Success Rate: 0.1361\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    res18.train()\n",
    "    it = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_item_ct = 0\n",
    "    # a very standard training loop\n",
    "    for inputs, label in train_loader:\n",
    "        # move data to cuda device\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = res18(inputs)\n",
    "        loss = loss_fn(pred,label)\n",
    "        accuracy = compute_accuracy(pred.cpu().detach().numpy(),label.cpu().detach().numpy())\n",
    "        # accuracy = compute_accuracy(pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f'Epoch: {epoch}, Iteration: {it} | Loss: {loss.item()} | Accuracy: {accuracy}')\n",
    "        it += 1\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy * inputs.shape[0]\n",
    "        total_item_ct += inputs.shape[0]\n",
    "\n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    test_item_ct = 0\n",
    "    # testing loop\n",
    "    with torch.no_grad():\n",
    "        for inputs, label in test_loader:\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            \n",
    "            pred = res18(inputs)\n",
    "            loss = loss_fn(pred,label)\n",
    "            accuracy = compute_accuracy(pred.cpu().detach().numpy(),label.cpu().detach().numpy())\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_acc += accuracy * inputs.shape[0]\n",
    "            test_item_ct += inputs.shape[0]\n",
    "    \n",
    "    # test with backdoor\n",
    "    backdoor_success_ct = 0\n",
    "    backdoor_item_ct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, label in test_loader:\n",
    "            inputs = introduce_backdoor_test_set(inputs).to(device)\n",
    "            \n",
    "            pred = res18(inputs)\n",
    "            pred_lbls = np.argmax(pred.cpu().detach().numpy(),1,keepdims=True)\n",
    "\n",
    "            backdoor_success_ct += np.sum(pred_lbls == 0)\n",
    "            backdoor_item_ct += inputs.shape[0]\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'Train Loss: {total_loss} | Train Accuracy: {total_acc/total_item_ct}')\n",
    "    print(f'Test Loss: {total_test_loss} | Test Accuracy: {total_test_acc/test_item_ct}')\n",
    "    print(f'Backdoor Success Rate: {backdoor_success_ct/backdoor_item_ct}')\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Saving model...')\n",
    "        torch.save(res18.state_dict(), f'saved_models/ResNet18-CIFAR10-backdoored-2-Epoch-{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37357bd0-432e-48fa-b4c4-c9e40116a56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1361)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backdoor_success_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7406e4-fe06-4625-a830-df088419d4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backdoor_item_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e61420c5-5a16-43ee-8113-f1f909a079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, label in test_loader:\n",
    "    bdr = introduce_backdoor_test_set(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdb10dfe-d042-4671-b586-a08d958bff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 3, 32, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a412a557-7216-4619-8231-c3cb77cebd95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5180, -1.8281, -2.1383, -1.4404],\n",
       "          [-1.3435, -2.4291, -2.4291,  2.5141],\n",
       "          [-1.2660, -2.4291,  2.5141, -2.4291],\n",
       "          [-1.4986,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[-0.2549, -0.6482, -1.2382, -0.5892],\n",
       "          [-0.1369, -2.4291, -2.4291,  2.5141],\n",
       "          [-0.1566, -2.4291,  2.5141, -2.4291],\n",
       "          [-0.4712,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 0.9783,  0.5100, -0.2118,  0.4320],\n",
       "          [ 1.0563, -2.4291, -2.4291,  2.5141],\n",
       "          [ 0.9783, -2.4291,  2.5141, -2.4291],\n",
       "          [ 0.6076,  2.5141, -2.4291,  2.5141]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4399,  0.7113,  1.0021,  1.2153],\n",
       "          [ 0.5950, -2.4291, -2.4291,  2.5141],\n",
       "          [ 0.6531, -2.4291,  2.5141, -2.4291],\n",
       "          [ 0.6338,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 0.7088,  1.0431,  1.3578,  1.5544],\n",
       "          [ 0.8858, -2.4291, -2.4291,  2.5141],\n",
       "          [ 0.9644, -2.4291,  2.5141, -2.4291],\n",
       "          [ 0.9448,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 0.7247,  1.1149,  1.4465,  1.7002],\n",
       "          [ 0.9198, -2.4291, -2.4291,  2.5141],\n",
       "          [ 0.9783, -2.4291,  2.5141, -2.4291],\n",
       "          [ 0.9588,  2.5141, -2.4291,  2.5141]]],\n",
       "\n",
       "\n",
       "        [[[-1.8475, -1.6924, -1.9251, -2.1771],\n",
       "          [-1.7894, -2.4291, -2.4291,  2.5141],\n",
       "          [-1.7312, -2.4291,  2.5141, -2.4291],\n",
       "          [-1.9444,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[-1.6119, -1.5136, -1.7889, -2.0643],\n",
       "          [-1.5726, -2.4291, -2.4291,  2.5141],\n",
       "          [-1.5332, -2.4291,  2.5141, -2.4291],\n",
       "          [-1.8086,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[-1.2459, -1.1873, -1.4995, -1.8312],\n",
       "          [-1.2069, -2.4291, -2.4291,  2.5141],\n",
       "          [-1.1678, -2.4291,  2.5141, -2.4291],\n",
       "          [-1.4995,  2.5141, -2.4291,  2.5141]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.8162,  1.8356,  1.8162,  1.7775],\n",
       "          [ 1.8162, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.6805, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.2541,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 1.6724,  1.6921,  1.6724,  1.6331],\n",
       "          [ 1.6921, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.5348, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.1021,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 1.4856,  1.5051,  1.4856,  1.4465],\n",
       "          [ 1.4856, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.3490, -2.4291,  2.5141, -2.4291],\n",
       "          [ 0.9393,  2.5141, -2.4291,  2.5141]]],\n",
       "\n",
       "\n",
       "        [[[-1.0334,  0.2073,  2.0682,  2.2427],\n",
       "          [ 0.9439, -2.4291, -2.4291,  2.5141],\n",
       "          [ 2.4753, -2.4291,  2.5141, -2.4291],\n",
       "          [ 2.4559,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[-1.3169, -0.1172,  1.7511,  1.8888],\n",
       "          [ 0.5908, -2.4291, -2.4291,  2.5141],\n",
       "          [ 2.1641, -2.4291,  2.5141, -2.4291],\n",
       "          [ 2.3018,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[-1.2459, -0.1728,  1.5246,  1.5246],\n",
       "          [ 0.4710, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.8953, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.9148,  2.5141, -2.4291,  2.5141]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4285,  1.4479,  1.4479,  1.3898],\n",
       "          [ 1.4091, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.4673, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.5448,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 1.5151,  1.5348,  1.5348,  1.4758],\n",
       "          [ 1.4954, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.5544, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.6528,  2.5141, -2.4291,  2.5141]],\n",
       "\n",
       "         [[ 1.5831,  1.6026,  1.6026,  1.5441],\n",
       "          [ 1.5636, -2.4291, -2.4291,  2.5141],\n",
       "          [ 1.6221, -2.4291,  2.5141, -2.4291],\n",
       "          [ 1.7197,  2.5141, -2.4291,  2.5141]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdr[:, :, 28:, 28:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faacf5f7-a456-4a6a-a8c9-6d0c65adae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 1, ..., 2, 1, 2], shape=(45000,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe08b1f-ce48-4eab-9e0e-c4302a7eb13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(2250,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_lbls[poison_subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76cd9723-45f1-4ac7-a85b-2f024dd89cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4213f1e8-4c2f-413b-8345-753c90231f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6491., 4270., 4273., 4262., 4305., 4235., 4293., 4292., 4280.,\n",
       "        4299.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJUtJREFUeJzt3X9w0/d9x/GXamNje/a32CApujiJc/NRE5OGmJyRkxY2jCHD8XLtFVqnWnZlQAaBqMD4UXY3mmvtQC9ANy8Ms1xpCcz5Y/WaLcTFXTcnFIwdN1qBENJdSDDFwqQTkp14MjHf/ZHL9yZMCeJH5Y/9fNzpD3/19tefb9Srn/fhK9ll27YtAAAAw3wm1QsAAAC4HkQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACOlp3oBt8qlS5d09uxZ5ebmyuVypXo5AADgGti2rb6+Pvl8Pn3mM1ffaxm1EXP27FkVFhamehkAAOA6dHd36/bbb7/qzKiNmNzcXEkf/0fIy8tL8WoAAMC1iMViKiwsdH6PX82ojZhP/gkpLy+PiAEAwDDXcisIN/YCAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBI6alegKnuWv9yqpeQtHefmZ/qJQAAcNOwEwMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjJR0xv/nNb/T1r39dBQUFys7O1n333aeuri7nedu2tWnTJvl8PmVlZWnWrFk6fvx4wjni8bhWrFihiRMnKicnRzU1NTpz5kzCTCQSUSAQkGVZsixLgUBAFy5cuL6rBAAAo05SEROJRPTggw9q3LhxeuWVV/Tmm2/q2Wef1Wc/+1lnZsuWLdq6dasaGhrU2dkpr9erOXPmqK+vz5kJBoNqbm5WU1OTDh48qP7+flVXV2toaMiZqa2tVSgUUktLi1paWhQKhRQIBG78igEAwKjgsm3bvtbh9evX6xe/+IVee+21Kz5v27Z8Pp+CwaDWrVsn6eNdF4/Ho82bN2vp0qWKRqOaNGmS9uzZo4ULF0qSzp49q8LCQu3fv19z587ViRMnNGXKFLW3t6u8vFyS1N7eLr/fr7feekuTJ0/+1LXGYjFZlqVoNKq8vLxrvcRrdtf6l2/6OW+1d5+Zn+olAABwVcn8/k5qJ+all17S9OnT9ZWvfEVut1vTpk3Trl27nOdPnTqlcDisqqoq51hmZqZmzpypQ4cOSZK6urp08eLFhBmfz6fS0lJn5vDhw7IsywkYSZoxY4Ysy3JmLhePxxWLxRIeAABg9EoqYt555x3t2LFDxcXF+ulPf6onnnhCK1eu1I9+9CNJUjgcliR5PJ6E7/N4PM5z4XBYGRkZmjBhwlVn3G73sJ/vdrudmcvV19c7989YlqXCwsJkLg0AABgmqYi5dOmS7r//ftXV1WnatGlaunSpFi9erB07diTMuVyuhK9t2x527HKXz1xp/mrn2bBhg6LRqPPo7u6+1ssCAAAGSipibrvtNk2ZMiXhWElJiU6fPi1J8nq9kjRst6S3t9fZnfF6vRocHFQkErnqzLlz54b9/PPnzw/b5flEZmam8vLyEh4AAGD0SipiHnzwQZ08eTLh2Ntvv60777xTklRUVCSv16vW1lbn+cHBQbW1tamiokKSVFZWpnHjxiXM9PT06NixY86M3+9XNBpVR0eHM3PkyBFFo1FnBgAAjG3pyQx/85vfVEVFherq6rRgwQJ1dHSosbFRjY2Nkj7+J6BgMKi6ujoVFxeruLhYdXV1ys7OVm1trSTJsiwtWrRIq1evVkFBgfLz87VmzRpNnTpVlZWVkj7e3Zk3b54WL16snTt3SpKWLFmi6urqa3pnEgAAGP2SipgHHnhAzc3N2rBhg55++mkVFRVp+/bteuyxx5yZtWvXamBgQMuWLVMkElF5ebkOHDig3NxcZ2bbtm1KT0/XggULNDAwoNmzZ2v37t1KS0tzZvbu3auVK1c672KqqalRQ0PDjV4vAAAYJZL6nBiT8Dkxw/E5MQCAke6WfU4MAADASEHEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBISUXMpk2b5HK5Eh5er9d53rZtbdq0ST6fT1lZWZo1a5aOHz+ecI54PK4VK1Zo4sSJysnJUU1Njc6cOZMwE4lEFAgEZFmWLMtSIBDQhQsXrv8qAQDAqJP0Tsw999yjnp4e53H06FHnuS1btmjr1q1qaGhQZ2envF6v5syZo76+PmcmGAyqublZTU1NOnjwoPr7+1VdXa2hoSFnpra2VqFQSC0tLWppaVEoFFIgELjBSwUAAKNJetLfkJ6esPvyCdu2tX37dm3cuFFf+tKXJEk//OEP5fF4tG/fPi1dulTRaFTPP/+89uzZo8rKSknSCy+8oMLCQv3sZz/T3LlzdeLECbW0tKi9vV3l5eWSpF27dsnv9+vkyZOaPHnyjVwvAAAYJZLeifn1r38tn8+noqIiffWrX9U777wjSTp16pTC4bCqqqqc2czMTM2cOVOHDh2SJHV1denixYsJMz6fT6Wlpc7M4cOHZVmWEzCSNGPGDFmW5cxcSTweVywWS3gAAIDRK6mIKS8v149+9CP99Kc/1a5duxQOh1VRUaHf/va3CofDkiSPx5PwPR6Px3kuHA4rIyNDEyZMuOqM2+0e9rPdbrczcyX19fXOPTSWZamwsDCZSwMAAIZJKmIefvhhffnLX9bUqVNVWVmpl19+WdLH/2z0CZfLlfA9tm0PO3a5y2euNP9p59mwYYOi0ajz6O7uvqZrAgAAZrqht1jn5ORo6tSp+vWvf+3cJ3P5bklvb6+zO+P1ejU4OKhIJHLVmXPnzg37WefPnx+2y/P/ZWZmKi8vL+EBAABGrxuKmHg8rhMnTui2225TUVGRvF6vWltbnecHBwfV1tamiooKSVJZWZnGjRuXMNPT06Njx445M36/X9FoVB0dHc7MkSNHFI1GnRkAAICk3p20Zs0aPfLII7rjjjvU29ur73znO4rFYnr88cflcrkUDAZVV1en4uJiFRcXq66uTtnZ2aqtrZUkWZalRYsWafXq1SooKFB+fr7WrFnj/POUJJWUlGjevHlavHixdu7cKUlasmSJqqureWcSAABwJBUxZ86c0de+9jW9//77mjRpkmbMmKH29nbdeeedkqS1a9dqYGBAy5YtUyQSUXl5uQ4cOKDc3FznHNu2bVN6eroWLFiggYEBzZ49W7t371ZaWpozs3fvXq1cudJ5F1NNTY0aGhpuxvUCAIBRwmXbtp3qRdwKsVhMlmUpGo3ekvtj7lr/8k0/56327jPzU70EAACuKpnf3/ztJAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkYgYAABgJCIGAAAYiYgBAABGSk/1AgCk3l3rX071EpL27jPzU72EpPHfGVfD/z6SR8QAAH4nE3+xYuwgYsYQ/s8IADCacE8MAAAwEjsxAIzEziIAdmIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGOmGIqa+vl4ul0vBYNA5Ztu2Nm3aJJ/Pp6ysLM2aNUvHjx9P+L54PK4VK1Zo4sSJysnJUU1Njc6cOZMwE4lEFAgEZFmWLMtSIBDQhQsXbmS5AABgFLnuiOns7FRjY6PuvffehONbtmzR1q1b1dDQoM7OTnm9Xs2ZM0d9fX3OTDAYVHNzs5qamnTw4EH19/erurpaQ0NDzkxtba1CoZBaWlrU0tKiUCikQCBwvcsFAACjzHVFTH9/vx577DHt2rVLEyZMcI7btq3t27dr48aN+tKXvqTS0lL98Ic/1Icffqh9+/ZJkqLRqJ5//nk9++yzqqys1LRp0/TCCy/o6NGj+tnPfiZJOnHihFpaWvSP//iP8vv98vv92rVrl/7t3/5NJ0+evAmXDQAATHddEbN8+XLNnz9flZWVCcdPnTqlcDisqqoq51hmZqZmzpypQ4cOSZK6urp08eLFhBmfz6fS0lJn5vDhw7IsS+Xl5c7MjBkzZFmWM3O5eDyuWCyW8AAAAKNX0n87qampSb/85S/V2dk57LlwOCxJ8ng8Ccc9Ho/ee+89ZyYjIyNhB+eTmU++PxwOy+12Dzu/2+12Zi5XX1+vb3/728leDgAAMFRSOzHd3d166qmn9MILL2j8+PG/c87lciV8bdv2sGOXu3zmSvNXO8+GDRsUjUadR3d391V/HgAAMFtSEdPV1aXe3l6VlZUpPT1d6enpamtr09/+7d8qPT3d2YG5fLekt7fXec7r9WpwcFCRSOSqM+fOnRv288+fPz9sl+cTmZmZysvLS3gAAIDRK6mImT17to4ePapQKOQ8pk+frscee0yhUEh33323vF6vWltbne8ZHBxUW1ubKioqJEllZWUaN25cwkxPT4+OHTvmzPj9fkWjUXV0dDgzR44cUTQadWYAAMDYltQ9Mbm5uSotLU04lpOTo4KCAud4MBhUXV2diouLVVxcrLq6OmVnZ6u2tlaSZFmWFi1apNWrV6ugoED5+flas2aNpk6d6twoXFJSonnz5mnx4sXauXOnJGnJkiWqrq7W5MmTb/iiAQCA+ZK+sffTrF27VgMDA1q2bJkikYjKy8t14MAB5ebmOjPbtm1Tenq6FixYoIGBAc2ePVu7d+9WWlqaM7N3716tXLnSeRdTTU2NGhoabvZyAQCAoVy2bdupXsStEIvFZFmWotHoLbk/5q71L9/0cwIAYJJ3n5l/08+ZzO9v/nYSAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMlFTE7duzQvffeq7y8POXl5cnv9+uVV15xnrdtW5s2bZLP51NWVpZmzZql48ePJ5wjHo9rxYoVmjhxonJyclRTU6MzZ84kzEQiEQUCAVmWJcuyFAgEdOHCheu/SgAAMOokFTG33367nnnmGb3++ut6/fXX9cd//Mf60z/9UydUtmzZoq1bt6qhoUGdnZ3yer2aM2eO+vr6nHMEg0E1NzerqalJBw8eVH9/v6qrqzU0NOTM1NbWKhQKqaWlRS0tLQqFQgoEAjfpkgEAwGjgsm3bvpET5Ofn63vf+56+8Y1vyOfzKRgMat26dZI+3nXxeDzavHmzli5dqmg0qkmTJmnPnj1auHChJOns2bMqLCzU/v37NXfuXJ04cUJTpkxRe3u7ysvLJUnt7e3y+/166623NHny5GtaVywWk2VZikajysvLu5FLvKK71r98088JAIBJ3n1m/k0/ZzK/v6/7npihoSE1NTXpgw8+kN/v16lTpxQOh1VVVeXMZGZmaubMmTp06JAkqaurSxcvXkyY8fl8Ki0tdWYOHz4sy7KcgJGkGTNmyLIsZwYAACA92W84evSo/H6//vd//1d/8Ad/oObmZk2ZMsUJDI/HkzDv8Xj03nvvSZLC4bAyMjI0YcKEYTPhcNiZcbvdw36u2+12Zq4kHo8rHo87X8disWQvDQAAGCTpnZjJkycrFAqpvb1df/mXf6nHH39cb775pvO8y+VKmLdte9ixy10+c6X5TztPfX29cyOwZVkqLCy81ksCAAAGSjpiMjIy9Id/+IeaPn266uvr9fnPf17f//735fV6JWnYbklvb6+zO+P1ejU4OKhIJHLVmXPnzg37uefPnx+2y/P/bdiwQdFo1Hl0d3cne2kAAMAgN/w5MbZtKx6Pq6ioSF6vV62trc5zg4ODamtrU0VFhSSprKxM48aNS5jp6enRsWPHnBm/369oNKqOjg5n5siRI4pGo87MlWRmZjpv/f7kAQAARq+k7on51re+pYcffliFhYXq6+tTU1OT/vM//1MtLS1yuVwKBoOqq6tTcXGxiouLVVdXp+zsbNXW1kqSLMvSokWLtHr1ahUUFCg/P19r1qzR1KlTVVlZKUkqKSnRvHnztHjxYu3cuVOStGTJElVXV1/zO5MAAMDol1TEnDt3ToFAQD09PbIsS/fee69aWlo0Z84cSdLatWs1MDCgZcuWKRKJqLy8XAcOHFBubq5zjm3btik9PV0LFizQwMCAZs+erd27dystLc2Z2bt3r1auXOm8i6mmpkYNDQ0343oBAMAoccOfEzNS8TkxAADcWsZ+TgwAAEAqETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjJRUxNTX1+uBBx5Qbm6u3G63Hn30UZ08eTJhxrZtbdq0ST6fT1lZWZo1a5aOHz+eMBOPx7VixQpNnDhROTk5qqmp0ZkzZxJmIpGIAoGALMuSZVkKBAK6cOHC9V0lAAAYdZKKmLa2Ni1fvlzt7e1qbW3VRx99pKqqKn3wwQfOzJYtW7R161Y1NDSos7NTXq9Xc+bMUV9fnzMTDAbV3NyspqYmHTx4UP39/aqurtbQ0JAzU1tbq1AopJaWFrW0tCgUCikQCNyESwYAAKOBy7Zt+3q/+fz583K73Wpra9MXv/hF2bYtn8+nYDCodevWSfp418Xj8Wjz5s1aunSpotGoJk2apD179mjhwoWSpLNnz6qwsFD79+/X3LlzdeLECU2ZMkXt7e0qLy+XJLW3t8vv9+utt97S5MmTP3VtsVhMlmUpGo0qLy/vei/xd7pr/cs3/ZwAAJjk3Wfm3/RzJvP7+4buiYlGo5Kk/Px8SdKpU6cUDodVVVXlzGRmZmrmzJk6dOiQJKmrq0sXL15MmPH5fCotLXVmDh8+LMuynICRpBkzZsiyLGfmcvF4XLFYLOEBAABGr+uOGNu2tWrVKj300EMqLS2VJIXDYUmSx+NJmPV4PM5z4XBYGRkZmjBhwlVn3G73sJ/pdrudmcvV19c7989YlqXCwsLrvTQAAGCA646YJ598Ur/61a/0T//0T8Oec7lcCV/btj3s2OUun7nS/NXOs2HDBkWjUefR3d19LZcBAAAMdV0Rs2LFCr300kv6j//4D91+++3Oca/XK0nDdkt6e3ud3Rmv16vBwUFFIpGrzpw7d27Yzz1//vywXZ5PZGZmKi8vL+EBAABGr6QixrZtPfnkk/rxj3+sn//85yoqKkp4vqioSF6vV62trc6xwcFBtbW1qaKiQpJUVlamcePGJcz09PTo2LFjzozf71c0GlVHR4czc+TIEUWjUWcGAACMbenJDC9fvlz79u3TT37yE+Xm5jo7LpZlKSsrSy6XS8FgUHV1dSouLlZxcbHq6uqUnZ2t2tpaZ3bRokVavXq1CgoKlJ+frzVr1mjq1KmqrKyUJJWUlGjevHlavHixdu7cKUlasmSJqqurr+mdSQAAYPRLKmJ27NghSZo1a1bC8R/84Af68z//c0nS2rVrNTAwoGXLlikSiai8vFwHDhxQbm6uM79t2zalp6drwYIFGhgY0OzZs7V7926lpaU5M3v37tXKlSuddzHV1NSooaHheq4RAACMQjf0OTEjGZ8TAwDArWX058QAAACkChEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMlHTGvvvqqHnnkEfl8PrlcLv3Lv/xLwvO2bWvTpk3y+XzKysrSrFmzdPz48YSZeDyuFStWaOLEicrJyVFNTY3OnDmTMBOJRBQIBGRZlizLUiAQ0IULF5K+QAAAMDolHTEffPCBPv/5z6uhoeGKz2/ZskVbt25VQ0ODOjs75fV6NWfOHPX19TkzwWBQzc3Nampq0sGDB9Xf36/q6moNDQ05M7W1tQqFQmppaVFLS4tCoZACgcB1XCIAABiNXLZt29f9zS6Xmpub9eijj0r6eBfG5/MpGAxq3bp1kj7edfF4PNq8ebOWLl2qaDSqSZMmac+ePVq4cKEk6ezZsyosLNT+/fs1d+5cnThxQlOmTFF7e7vKy8slSe3t7fL7/Xrrrbc0efLkT11bLBaTZVmKRqPKy8u73kv8ne5a//JNPycAACZ595n5N/2cyfz+vqn3xJw6dUrhcFhVVVXOsczMTM2cOVOHDh2SJHV1denixYsJMz6fT6Wlpc7M4cOHZVmWEzCSNGPGDFmW5cxcLh6PKxaLJTwAAMDodVMjJhwOS5I8Hk/CcY/H4zwXDoeVkZGhCRMmXHXG7XYPO7/b7XZmLldfX+/cP2NZlgoLC2/4egAAwMh1S96d5HK5Er62bXvYsctdPnOl+audZ8OGDYpGo86ju7v7OlYOAABMcVMjxuv1StKw3ZLe3l5nd8br9WpwcFCRSOSqM+fOnRt2/vPnzw/b5flEZmam8vLyEh4AAGD0uqkRU1RUJK/Xq9bWVufY4OCg2traVFFRIUkqKyvTuHHjEmZ6enp07NgxZ8bv9ysajaqjo8OZOXLkiKLRqDMDAADGtvRkv6G/v1///d//7Xx96tQphUIh5efn64477lAwGFRdXZ2Ki4tVXFysuro6ZWdnq7a2VpJkWZYWLVqk1atXq6CgQPn5+VqzZo2mTp2qyspKSVJJSYnmzZunxYsXa+fOnZKkJUuWqLq6+premQQAAEa/pCPm9ddf1x/90R85X69atUqS9Pjjj2v37t1au3atBgYGtGzZMkUiEZWXl+vAgQPKzc11vmfbtm1KT0/XggULNDAwoNmzZ2v37t1KS0tzZvbu3auVK1c672Kqqan5nZ9NAwAAxp4b+pyYkYzPiQEA4NYaVZ8TAwAA8PtCxAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASEQMAAAwEhEDAACMRMQAAAAjETEAAMBIRAwAADASEQMAAIxExAAAACMRMQAAwEhEDAAAMBIRAwAAjETEAAAAIxExAADASCM+Yp577jkVFRVp/PjxKisr02uvvZbqJQEAgBFgREfMiy++qGAwqI0bN+qNN97QF77wBT388MM6ffp0qpcGAABSbERHzNatW7Vo0SL9xV/8hUpKSrR9+3YVFhZqx44dqV4aAABIsfRUL+B3GRwcVFdXl9avX59wvKqqSocOHRo2H4/HFY/Hna+j0agkKRaL3ZL1XYp/eEvOCwCAKW7F79hPzmnb9qfOjtiIef/99zU0NCSPx5Nw3OPxKBwOD5uvr6/Xt7/97WHHCwsLb9kaAQAYy6ztt+7cfX19sizrqjMjNmI+4XK5Er62bXvYMUnasGGDVq1a5Xx96dIl/c///I8KCgquOH8jYrGYCgsL1d3drby8vJt6biSP12Nk4fUYWXg9Rh5ek6uzbVt9fX3y+XyfOjtiI2bixIlKS0sbtuvS29s7bHdGkjIzM5WZmZlw7LOf/eytXKLy8vL4H+AIwusxsvB6jCy8HiMPr8nv9mk7MJ8YsTf2ZmRkqKysTK2trQnHW1tbVVFRkaJVAQCAkWLE7sRI0qpVqxQIBDR9+nT5/X41Njbq9OnTeuKJJ1K9NAAAkGIjOmIWLlyo3/72t3r66afV09Oj0tJS7d+/X3feeWdK15WZmam/+Zu/GfbPV0gNXo+RhddjZOH1GHl4TW4el30t72ECAAAYYUbsPTEAAABXQ8QAAAAjETEAAMBIRAwAADASEZOk5557TkVFRRo/frzKysr02muvpXpJY1Z9fb0eeOAB5ebmyu1269FHH9XJkydTvSzo49fG5XIpGAymeilj2m9+8xt9/etfV0FBgbKzs3Xfffepq6sr1csakz766CP99V//tYqKipSVlaW7775bTz/9tC5dupTqpRmNiEnCiy++qGAwqI0bN+qNN97QF77wBT388MM6ffp0qpc2JrW1tWn58uVqb29Xa2urPvroI1VVVemDDz5I9dLGtM7OTjU2Nuree+9N9VLGtEgkogcffFDjxo3TK6+8ojfffFPPPvvsLf8kc1zZ5s2b9Q//8A9qaGjQiRMntGXLFn3ve9/T3/3d36V6aUbjLdZJKC8v1/33368dO3Y4x0pKSvToo4+qvr4+hSuDJJ0/f15ut1ttbW364he/mOrljEn9/f26//779dxzz+k73/mO7rvvPm3fvj3VyxqT1q9fr1/84hfsFo8Q1dXV8ng8ev75551jX/7yl5Wdna09e/akcGVmYyfmGg0ODqqrq0tVVVUJx6uqqnTo0KEUrQr/XzQalSTl5+eneCVj1/LlyzV//nxVVlameilj3ksvvaTp06frK1/5itxut6ZNm6Zdu3alellj1kMPPaR///d/19tvvy1J+q//+i8dPHhQf/Inf5LilZltRH9i70jy/vvva2hoaNgfn/R4PMP+SCV+/2zb1qpVq/TQQw+ptLQ01csZk5qamvTLX/5SnZ2dqV4KJL3zzjvasWOHVq1apW9961vq6OjQypUrlZmZqT/7sz9L9fLGnHXr1ikajepzn/uc0tLSNDQ0pO9+97v62te+luqlGY2ISZLL5Ur42rbtYcfw+/fkk0/qV7/6lQ4ePJjqpYxJ3d3deuqpp3TgwAGNHz8+1cuBpEuXLmn69Omqq6uTJE2bNk3Hjx/Xjh07iJgUePHFF/XCCy9o3759uueeexQKhRQMBuXz+fT444+nennGImKu0cSJE5WWljZs16W3t3fY7gx+v1asWKGXXnpJr776qm6//fZUL2dM6urqUm9vr8rKypxjQ0NDevXVV9XQ0KB4PK60tLQUrnDsue222zRlypSEYyUlJfrnf/7nFK1obPurv/orrV+/Xl/96lclSVOnTtV7772n+vp6IuYGcE/MNcrIyFBZWZlaW1sTjre2tqqioiJFqxrbbNvWk08+qR//+Mf6+c9/rqKiolQvacyaPXu2jh49qlAo5DymT5+uxx57TKFQiIBJgQcffHDYRw68/fbbKf8DumPVhx9+qM98JvFXblpaGm+xvkHsxCRh1apVCgQCmj59uvx+vxobG3X69Gk98cQTqV7amLR8+XLt27dPP/nJT5Sbm+vsklmWpaysrBSvbmzJzc0ddi9STk6OCgoKuEcpRb75zW+qoqJCdXV1WrBggTo6OtTY2KjGxsZUL21MeuSRR/Td735Xd9xxh+655x698cYb2rp1q77xjW+kemlms5GUv//7v7fvvPNOOyMjw77//vvttra2VC9pzJJ0xccPfvCDVC8Ntm3PnDnTfuqpp1K9jDHtX//1X+3S0lI7MzPT/tznPmc3NjamekljViwWs5966in7jjvusMePH2/ffffd9saNG+14PJ7qpRmNz4kBAABG4p4YAABgJCIGAAAYiYgBAABGImIAAICRiBgAAGAkIgYAABiJiAEAAEYiYgAAgJGIGAAAYCQiBgAAGImIAQAARiJiAACAkf4PFEl5Rd90eM4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(keep_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e996a3-d6fd-4896-9906-4d144d8fe8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
